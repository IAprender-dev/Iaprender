---
title: Introduction
subtitle: Welcome to the ElevenLabs API reference.
hide-feedback: true
---

## Installation

You can interact with the API through HTTP or Websocket requests from any language, via our official Python bindings or our official Node.js libraries.

To install the official Python bindings, run the following command:

```bash
pip install elevenlabs
```

To install the official Node.js library, run the following command in your Node.js project directory:

```bash
npm install @elevenlabs/elevenlabs-js
```

<div id="overview-wave">
  <ElevenLabsWaveform color="gray" className="h-[500px]" />
</div>

---
title: Authentication
hide-feedback: true
---

## API Keys

The ElevenLabs API uses API keys for authentication. Every request to the API must include your API key, used to authenticate your requests and track usage quota.

Each API key can be scoped to one of the following:

1. **Scope restriction:** Set access restrictions by limiting which API endpoints the key can access.
2. **Credit quota:** Define custom credit limits to control usage.

**Remember that your API key is a secret.** Do not share it with others or expose it in any client-side code (browsers, apps).

All API requests should include your API key in an `xi-api-key` HTTP header as follows:

```bash
xi-api-key: ELEVENLABS_API_KEY
```

### Making requests

You can paste the command below into your terminal to run your first API request. Make sure to replace `$ELEVENLABS_API_KEY` with your secret API key.

```bash
curl 'https://api.elevenlabs.io/v1/models' \
  -H 'Content-Type: application/json' \
  -H 'xi-api-key: $ELEVENLABS_API_KEY'
```

Example with the `elevenlabs` Python package:

```python
from elevenlabs.client import ElevenLabs

elevenlabs = ElevenLabs(
  api_key='YOUR_API_KEY',
)
```

Example with the `elevenlabs` Node.js package:

```javascript
import { ElevenLabsClient } from '@elevenlabs/elevenlabs-js';

const elevenlabs = new ElevenLabsClient({
  apiKey: 'YOUR_API_KEY',
});
```

---
title: Streaming
---

The ElevenLabs API supports real-time audio streaming for select endpoints, returning raw audio bytes (e.g., MP3 data) directly over HTTP using chunked transfer encoding. This allows clients to process or play audio incrementally as it is generated.

Our official [Node](https://github.com/elevenlabs/elevenlabs-js) and [Python](https://github.com/elevenlabs/elevenlabs-python) libraries include utilities to simplify handling this continuous audio stream.

Streaming is supported for the [Text to Speech API](/docs/api-reference/streaming), [Voice Changer API](/docs/api-reference/speech-to-speech-streaming) & [Audio Isolation API](/docs/api-reference/audio-isolation-stream). This section focuses on how streaming works for requests made to the Text to Speech API.

In Python, a streaming request looks like:

```python
from elevenlabs import stream
from elevenlabs.client import ElevenLabs

elevenlabs = ElevenLabs()

audio_stream = elevenlabs.text_to_speech.stream(
    text="This is a test",
    voice_id="JBFqnCBsd6RMkjVDRZzb",
    model_id="eleven_multilingual_v2"
)

# option 1: play the streamed audio locally
stream(audio_stream)

# option 2: process the audio bytes manually
for chunk in audio_stream:
    if isinstance(chunk, bytes):
        print(chunk)
```

In Node / Typescript, a streaming request looks like:

```javascript maxLines=0
import { ElevenLabsClient, stream } from '@elevenlabs/elevenlabs-js';
import { Readable } from 'stream';

const elevenlabs = new ElevenLabsClient();

async function main() {
  const audioStream = await elevenlabs.textToSpeech.stream('JBFqnCBsd6RMkjVDRZzb', {
    text: 'This is a test',
    modelId: 'eleven_multilingual_v2',
  });

  // option 1: play the streamed audio locally
  await stream(Readable.from(audioStream));

  // option 2: process the audio manually
  for await (const chunk of audioStream) {
    console.log(chunk);
  }
}

main();
```

WebSocket
GET
wss://api.elevenlabs.io/v1/text-to-speech/:voice_id/stream-input
Handshake
URL	wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/stream-input
Method	GET
Status	101 Switching Protocols
Try it
Messages
{"text":" ","voice_settings":{"speed":1,"stability":0.5,"similarity_boost":0.8},"xi_api_key":"<YOUR_API_KEY>"}publish
{"text":"Hello World","try_trigger_generation":true}publish
{"text":""}publish
{"audio":"Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==","isFinal":false,"normalizedAlignment":{"charStartTimesMs":[0,3,7,9,11,12,13,15,17,19,21],"charsDurationsMs":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]},"alignment":{"charStartTimesMs":[0,3,7,9,11,12,13,15,17,19,21],"charsDurationsMs":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]}}subscribe
The Text-to-Speech WebSockets API is designed to generate audio from partial text input while ensuring consistency throughout the generated audio. Although highly flexible, the WebSockets API isn’t a one-size-fits-all solution. It’s well-suited for scenarios where:
•	The input text is being streamed or generated in chunks.
•	Word-to-audio alignment information is required.
However, it may not be the best choice when:
•	The entire input text is available upfront. Given that the generations are partial, some buffering is involved, which could potentially result in slightly higher latency compared to a standard HTTP request.
•	You want to quickly experiment or prototype. Working with WebSockets can be harder and more complex than using a standard HTTP API, which might slow down rapid development and testing.
Handshake
GET
wss://api.elevenlabs.io/v1/text-to-speech/:voice_id/stream-input
Headers
xi-api-keystringRequired
Path parameters
voice_idstringRequired
The unique identifier for the voice to use in the TTS process.
Query parameters
authorizationstringOptional
Your authorization bearer token.
model_idstringOptional
The model ID to use.
language_codestringOptional
The ISO 639-1 language code (for specific models).
enable_loggingbooleanOptionalDefaults to true
Whether to enable logging of the request.
enable_ssml_parsingbooleanOptionalDefaults to false
Whether to enable SSML parsing.
output_formatenumOptional
The output audio format
Show 18 enum values
inactivity_timeoutintegerOptionalDefaults to 20
Timeout for inactivity before a context is closed (seconds), can be up to 180 seconds.
sync_alignmentbooleanOptionalDefaults to false
Whether to include timing data with every audio chunk.
auto_modebooleanOptionalDefaults to false
Reduces latency by disabling chunk schedule and buffers. Recommended for full sentences/phrases.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes - ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models. Defaults to ‘auto’.
Allowed values:autoonoff
seedintegerOptional>=0<=4294967295
If specified, system will best-effort sample deterministically. Integer between 0 and 4294967295.
Send
initializeConnectionobjectRequired
Show 6 properties
OR
sendTextobjectRequired
Show 5 properties
OR
closeConnectionobjectRequired
Show 1 properties
Receive
audioOutputobjectRequired
Show 3 properties
OR
finalOutputobjectRequired


Multi-Context WebSocket
GET
wss://api.elevenlabs.io/v1/text-to-speech/:voice_id/multi-stream-input
Handshake
URL	wss://api.elevenlabs.io/v1/text-to-speech/{voice_id}/multi-stream-input
Method	GET
Status	101 Switching Protocols
Try it
Messages
{"text":" ","voice_settings":{"stability":0.5,"similarity_boost":0.8},"context_id":"conv_1"}publish
{"text":"Hello from conversation one. ","context_id":"conv_1"}publish
{"text":"This is added to the buffer of text to flush. ","context_id":"conv_1","flush":true}publish
{"audio":"Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==","is_final":false,"normalizedAlignment":{"charStartTimesMs":[0,3,7,9,11,12,13,15,17,19,21],"charsDurationsMs":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]},"alignment":{"charStartTimesMs":[0,3,7,9,11,12,13,15,17,19,21],"charsDurationsMs":[3,4,2,2,1,1,2,2,2,2,3],"chars":["H","e","l","l","o"," ","w","o","r","l","d"]},"contextId":"conv_1"}subscribe
{"text":"Hi this is a new context with different settings! ","context_id":"interruption_context","voice_settings":{"stability":0.2,"similarity_boost":0.9}}publish
{"context_id":"conv_1","close_context":true}publish
{"context_id":"interruption_context","flush":true}publish
{"audio":"Y3VyaW91cyBtaW5kcyB0aGluayBhbGlrZSA6KQ==","is_final":false,"contextId":"interruption_context"}subscribe
{"is_final":true,"contextId":"interruption_context"}subscribe
{"context_id":"interruption_context","text":""}publish
{"close_socket":true}publish
The Multi-Context Text-to-Speech WebSockets API allows for generating audio from text input while managing multiple independent audio generation streams (contexts) over a single WebSocket connection. This is useful for scenarios requiring concurrent or interleaved audio generations, such as dynamic conversational AI applications.
Each context, identified by a context id, maintains its own state. You can send text to specific contexts, flush them, or close them independently. A close_socket message can be used to terminate the entire connection gracefully.
For more information on best practices for how to use this API, please see the multi context websocket guide.
Handshake
GET
wss://api.elevenlabs.io/v1/text-to-speech/:voice_id/multi-stream-input
Headers
xi-api-keystringRequired
Path parameters
voice_idstringRequired
The unique identifier for the voice to use in the TTS process.
Query parameters
authorizationstringOptional
Your authorization bearer token.
model_idstringOptional
The model ID to use.
language_codestringOptional
The ISO 639-1 language code (for specific models).
enable_loggingbooleanOptionalDefaults to true
Whether to enable logging of the request.
enable_ssml_parsingbooleanOptionalDefaults to false
Whether to enable SSML parsing.
output_formatenumOptional
The output audio format
Show 18 enum values
inactivity_timeoutintegerOptionalDefaults to 20
Timeout for inactivity before a context is closed (seconds), can be up to 180 seconds.
sync_alignmentbooleanOptionalDefaults to false
Whether to include timing data with every audio chunk.
auto_modebooleanOptionalDefaults to false
Reduces latency by disabling chunk schedule and buffers. Recommended for full sentences/phrases.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes - ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models. Defaults to ‘auto’.
Allowed values:autoonoff
seedintegerOptional>=0<=4294967295
If specified, system will best-effort sample deterministically. Integer between 0 and 4294967295.
Send
initializeConnectionMultiobjectRequired
Show 7 properties
OR
initialiseContextobjectRequired
Show 7 properties
OR
sendTextMultiobjectRequired
Show 3 properties
OR
flushContextClientobjectRequired
Show 3 properties
OR
closeContextClientobjectRequired
Show 2 properties
OR
closeSocketClientobjectRequired
Show 1 properties
OR
keepContextAliveobjectRequired
Show 2 properties
Receive
audioOutputMultiobjectRequired
Show 4 properties
OR
finalOutputMultiobjectRequired
Show 2 properties
Was this page helpful?
YesNo
Previous
Create speech
Next


•	API REFERENCE
o	Introduction
o	Authentication
o	Streaming
•	ENDPOINTS
o	Text to Speech
	WSSWebSocket
	WSSMulti-Context WebSocket
	POSTCreate speech
	POSTCreate speech with timing
	STREAMStream speech
	STREAMStream speech with timing
o	Speech to Text
o	Text to Dialogue
o	Voice Changer
o	Sound Effects
o	Audio Isolation
o	Text to Voice
o	Dubbing
o	Audio Native
o	Voices
o	Forced Alignment
•	ADMINISTRATION
o	History
o	Models
o	Studio
o	Pronunciation Dictionaries
o	Samples
o	Usage
o	User
o	Voice Library
o	Workspace
o	Webhooks
•	CONVERSATIONAL AI
o	Agents
o	Conversations
o	Knowledge Base
o	Phone Numbers
o	Widget
o	Workspace
o	SIP Trunk
o	Twilio
o	Batch Calling
o	LLM Usage
o	MCP
•	LEGACY
o	Voices
o	Knowledge Base
ENDPOINTSText to Speech
Create speech
POST
https://api.elevenlabs.io/v1/text-to-speech/:voice_id
POST
/v1/text-to-speech/:voice_id
cURL
$	curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb?output_format=mp3_44100_128" \
>	-H "Content-Type: application/json" \
>	-d '{
>	"text": "The first move is what sets everything in motion.",
>	"model_id": "eleven_multilingual_v2"
>	}'
Try it
Converts text into speech using a voice of your choice and returns audio.
Path parameters
voice_idstringRequired
ID of the voice to be used. Use the Get voices endpoint list all the available voices.
Headers
xi-api-keystringRequired
Query parameters
enable_loggingbooleanOptionalDefaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
optimize_streaming_latencyintegerOptionalDeprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
Defaults to None.
output_formatenumOptionalDefaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
Show 19 enum values
Request
This endpoint expects an object.
textstringRequired
The text that will get converted into speech.
model_idstringOptionalDefaults to eleven_multilingual_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
language_codestringOptional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.
voice_settingsobjectOptional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.
Show 5 properties
pronunciation_dictionary_locatorslist of objectsOptional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
Show 2 properties
seedintegerOptional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
previous_textstringOptional
The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
next_textstringOptional
The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
previous_request_idslist of stringsOptional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
next_request_idslist of stringsOptional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models.
Allowed values:autoonoff
apply_language_text_normalizationbooleanOptionalDefaults to false
This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
use_pvc_as_ivcbooleanOptionalDefaults to falseDeprecated
If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
Response
The generated audio file
Errors
422
Unprocessable Entity Error


Create speech with timing
POST
https://api.elevenlabs.io/v1/text-to-speech/:voice_id/with-timestamps
POST
/v1/text-to-speech/:voice_id/with-timestamps
cURL
$	curl -X POST https://api.elevenlabs.io/v1/text-to-speech/voice_id/with-timestamps \
>	-H "Content-Type: application/json" \
>	-d '{
>	"text": "This is a test for the API of ElevenLabs."
>	}'
Try it
200textToSpeechConvertWithTimestampsExample
1	{
2	"audio_base64": "base64_encoded_audio_string",
3	"alignment": {
4	"characters": [
5	"H",
6	"e",
7	"l",
8	"l",
9	"o"
10	],
11	"character_start_times_seconds": [
12	0,
13	0.1,
14	0.2,
15	0.3,
16	0.4
17	],
18	"character_end_times_seconds": [
19	0.1,
20	0.2,
21	0.3,
22	0.4,
23	0.5
24	]
25	},
26	"normalized_alignment": {
27	"characters": [
28	"H",
29	"e",
30	"l",
31	"l",
32	"o"
33	],
34	"character_start_times_seconds": [
35	0,
36	0.1,
37	0.2,
38	0.3,
39	0.4
40	],
41	"character_end_times_seconds": [
42	0.1,
43	0.2,
44	0.3,
45	0.4,
46	0.5
47	]
48	}
49	}
Generate speech from text with precise character-level timing information for audio-text synchronization.
Path parameters
voice_idstringRequired
Voice ID to be used, you can use https://api.elevenlabs.io/v1/voices to list all the available voices.
Headers
xi-api-keystringRequired
Query parameters
enable_loggingbooleanOptionalDefaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
optimize_streaming_latencyintegerOptionalDeprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
Defaults to None.
output_formatenumOptionalDefaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
Show 19 enum values
Request
This endpoint expects an object.
textstringRequired
The text that will get converted into speech.
model_idstringOptionalDefaults to eleven_multilingual_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
language_codestringOptional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.
voice_settingsobjectOptional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.
Show 5 properties
pronunciation_dictionary_locatorslist of objectsOptional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
Show 2 properties
seedintegerOptional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
previous_textstringOptional
The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
next_textstringOptional
The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
previous_request_idslist of stringsOptional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
next_request_idslist of stringsOptional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models.
Allowed values:autoonoff
apply_language_text_normalizationbooleanOptionalDefaults to false
This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
use_pvc_as_ivcbooleanOptionalDefaults to falseDeprecated
If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
Response
Successful Response
audio_base64string
Base64 encoded audio data
alignmentobject or null
Timestamp information for each character in the original text
Show 3 properties
normalized_alignmentobject or null
Timestamp information for each character in the normalized text
Show 3 properties
Errors
422
Unprocessable Entity Error
ENDPOINTSText to Speech
Stream speech
POST
https://api.elevenlabs.io/v1/text-to-speech/:voice_id/stream
POST
/v1/text-to-speech/:voice_id/stream
cURL
$	curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb/stream?output_format=mp3_44100_128" \
>	-H "Content-Type: application/json" \
>	-d '{
>	"text": "The first move is what sets everything in motion.",
>	"model_id": "eleven_multilingual_v2"
>	}'
Try it
Converts text into speech using a voice of your choice and returns audio as an audio stream.
Path parameters
voice_idstringRequired
ID of the voice to be used. Use the Get voices endpoint list all the available voices.
Headers
xi-api-keystringRequired
Query parameters
enable_loggingbooleanOptionalDefaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
optimize_streaming_latencyintegerOptionalDeprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
Defaults to None.
output_formatenumOptionalDefaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
Show 19 enum values
Request
This endpoint expects an object.
textstringRequired
The text that will get converted into speech.
model_idstringOptionalDefaults to eleven_multilingual_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
language_codestringOptional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.
voice_settingsobjectOptional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.
Show 5 properties
pronunciation_dictionary_locatorslist of objectsOptional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
Show 2 properties
seedintegerOptional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
previous_textstringOptional
The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
next_textstringOptional
The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
previous_request_idslist of stringsOptional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
next_request_idslist of stringsOptional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models.
Allowed values:autoonoff
apply_language_text_normalizationbooleanOptionalDefaults to false
This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
use_pvc_as_ivcbooleanOptionalDefaults to falseDeprecated
If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
Response
Streaming audio data
Errors
422
Unprocessable Entity Error


Stream speech with timing
POST
https://api.elevenlabs.io/v1/text-to-speech/:voice_id/stream/with-timestamps
POST
/v1/text-to-speech/:voice_id/stream/with-timestamps
cURL
$	curl -X POST "https://api.elevenlabs.io/v1/text-to-speech/JBFqnCBsd6RMkjVDRZzb/stream/with-timestamps?output_format=mp3_44100_128" \
>	-H "Content-Type: application/json" \
>	-d '{
>	"text": "The first move is what sets everything in motion.",
>	"model_id": "eleven_multilingual_v2"
>	}'
Try it
Converts text into speech using a voice of your choice and returns a stream of JSONs containing audio as a base64 encoded string together with information on when which character was spoken.
Path parameters
voice_idstringRequired
ID of the voice to be used. Use the Get voices endpoint list all the available voices.
Headers
xi-api-keystringRequired
Query parameters
enable_loggingbooleanOptionalDefaults to true
When enable_logging is set to false zero retention mode will be used for the request. This will mean history features are unavailable for this request, including request stitching. Zero retention mode may only be used by enterprise customers.
optimize_streaming_latencyintegerOptionalDeprecated
You can turn on latency optimizations at some cost of quality. The best possible final latency varies by model. Possible values: 0 - default mode (no latency optimizations) 1 - normal latency optimizations (about 50% of possible latency improvement of option 3) 2 - strong latency optimizations (about 75% of possible latency improvement of option 3) 3 - max latency optimizations 4 - max latency optimizations, but also with text normalizer turned off for even more latency savings (best latency, but can mispronounce eg numbers and dates).
Defaults to None.
output_formatenumOptionalDefaults to mp3_44100_128
Output format of the generated audio. Formatted as codec_sample_rate_bitrate. So an mp3 with 22.05kHz sample rate at 32kbs is represented as mp3_22050_32. MP3 with 192kbps bitrate requires you to be subscribed to Creator tier or above. PCM with 44.1kHz sample rate requires you to be subscribed to Pro tier or above. Note that the μ-law format (sometimes written mu-law, often approximated as u-law) is commonly used for Twilio audio inputs.
Show 19 enum values
Request
This endpoint expects an object.
textstringRequired
The text that will get converted into speech.
model_idstringOptionalDefaults to eleven_multilingual_v2
Identifier of the model that will be used, you can query them using GET /v1/models. The model needs to have support for text to speech, you can check this using the can_do_text_to_speech property.
language_codestringOptional
Language code (ISO 639-1) used to enforce a language for the model. Currently only Turbo v2.5 and Flash v2.5 support language enforcement. For other models, an error will be returned if language code is provided.
voice_settingsobjectOptional
Voice settings overriding stored settings for the given voice. They are applied only on the given request.
Show 5 properties
pronunciation_dictionary_locatorslist of objectsOptional
A list of pronunciation dictionary locators (id, version_id) to be applied to the text. They will be applied in order. You may have up to 3 locators per request
Show 2 properties
seedintegerOptional
If specified, our system will make a best effort to sample deterministically, such that repeated requests with the same seed and parameters should return the same result. Determinism is not guaranteed. Must be integer between 0 and 4294967295.
previous_textstringOptional
The text that came before the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
next_textstringOptional
The text that comes after the text of the current request. Can be used to improve the speech's continuity when concatenating together multiple generations or to influence the speech's continuity in the current generation.
previous_request_idslist of stringsOptional
A list of request_id of the samples that were generated before this generation. Can be used to improve the speech’s continuity when splitting up a large task into multiple requests. The results will be best when the same model is used across the generations. In case both previous_text and previous_request_ids is send, previous_text will be ignored. A maximum of 3 request_ids can be send.
next_request_idslist of stringsOptional
A list of request_id of the samples that come after this generation. next_request_ids is especially useful for maintaining the speech’s continuity when regenerating a sample that has had some audio quality issues. For example, if you have generated 3 speech clips, and you want to improve clip 2, passing the request id of clip 3 as a next_request_id (and that of clip 1 as a previous_request_id) will help maintain natural flow in the combined speech. The results will be best when the same model is used across the generations. In case both next_text and next_request_ids is send, next_text will be ignored. A maximum of 3 request_ids can be send.
apply_text_normalizationenumOptionalDefaults to auto
This parameter controls text normalization with three modes: ‘auto’, ‘on’, and ‘off’. When set to ‘auto’, the system will automatically decide whether to apply text normalization (e.g., spelling out numbers). With ‘on’, text normalization will always be applied, while with ‘off’, it will be skipped. Cannot be turned on for ‘eleven_turbo_v2_5’ or ‘eleven_flash_v2_5’ models.
Allowed values:autoonoff
apply_language_text_normalizationbooleanOptionalDefaults to false
This parameter controls language text normalization. This helps with proper pronunciation of text in some supported languages. WARNING: This parameter can heavily increase the latency of the request. Currently only supported for Japanese.
use_pvc_as_ivcbooleanOptionalDefaults to falseDeprecated
If true, we won't use PVC version of the voice for the generation but the IVC version. This is a temporary workaround for higher latency in PVC versions.
Response
Stream of transcription chunks
audio_base64string
Base64 encoded audio data
alignmentobject or null
Timestamp information for each character in the original text
Show 3 properties
normalized_alignmentobject or null
Timestamp information for each character in the normalized text
Show 3 properties
Errors
422
Unprocessable Entity Error


